A Comprehensive Text Processing Strategy for Vietnamese RAG Application Using Underthesea and BKAi Bi-Encoder
Introduction: Setting the Stage for Vietnamese RAG with Underthesea and BKAi Bi-Encoder
Retrieval-Augmented Generation (RAG) represents a significant advancement in the field of natural language processing, particularly for languages like Vietnamese where the availability of extensive, high-quality training datasets for large language models might be less abundant than for English. RAG enhances the capabilities of language models by enabling them to access and incorporate information from external knowledge sources during the text generation process. This approach is especially beneficial for knowledge-intensive tasks in Vietnamese, allowing for more accurate, contextually relevant, and informative responses grounded in factual data.
The success of any RAG application is heavily dependent on the effectiveness of the text preprocessing stage. For a language like Vietnamese, which possesses unique linguistic characteristics such as its agglutinative nature and tonal system, meticulous preprocessing is paramount. Proper handling of text before it is fed into the retrieval and generation components ensures that the underlying models can accurately understand and process the information. This includes tasks like standardizing the text, segmenting it into meaningful units, and preparing it for embedding.
In the context of Vietnamese natural language processing, the underthesea NLP toolkit stands out as a robust and versatile open-source Python library 1. This toolkit offers a wide array of functionalities specifically designed for Vietnamese, including word segmentation, text normalization, part-of-speech tagging, and named entity recognition. Its active development and widespread adoption within the Vietnamese NLP community make it a reliable and valuable resource for developers 1.
For the crucial task of generating dense vector embeddings, the bkai-foundation-models/vietnamese-bi-encoder has been chosen 4. This model is specifically trained on Vietnamese text and excels at mapping sentences and paragraphs into a high-dimensional semantic space. These embeddings are essential for enabling efficient similarity search and retrieval of relevant documents from the knowledge base. The model's training on a diverse Vietnamese dataset, which includes translated versions of MS MARCO and SQuAD, as well as a significant portion of the Legal Text Retrieval Zalo 2021 challenge dataset, suggests its strong capabilities in understanding the semantic nuances of Vietnamese text across various domains 4. This broad training makes it potentially well-suited for handling data originating from Google Docs and Sheets, which can contain diverse content.
This report aims to provide a comprehensive and tailored text preprocessing strategy for the user's Vietnamese RAG application. The strategy will focus on effectively utilizing the functionalities offered by the underthesea toolkit while strictly adhering to the constraints imposed by the vietnamese-bi-encoder, particularly its maximum input sequence length. The ultimate goal is to ensure optimal performance of the RAG application by preparing the Vietnamese text data in the most suitable manner for embedding and retrieval.
Understanding the Constraints and Requirements
A critical aspect of developing an effective text preprocessing strategy is a thorough understanding of the limitations and requirements of the chosen embedding model. In this case, the bkai-foundation-models/vietnamese-bi-encoder imposes specific constraints that must be carefully considered.
The vietnamese-bi-encoder has a strict maximum input sequence length of 128 tokens 4. This limitation has significant implications for the RAG application. Any text input that exceeds this token count will likely be truncated by the model. Truncation can lead to a loss of crucial context, especially if important information resides towards the end of a long document or sentence. Consequently, the quality of the generated embeddings might be compromised, and the accuracy of the retrieval process could be negatively affected. Therefore, a robust strategy for breaking down the source documents into smaller, manageable chunks, each adhering to this token limit, is essential. This will likely involve sentence segmentation followed by further chunking if necessary.
Furthermore, the vietnamese-bi-encoder requires the input Vietnamese text to be pre-segmented into individual words 4. Unlike English, where spaces typically delineate word boundaries, Vietnamese often requires additional processing to accurately identify words. Word segmentation is a fundamental step for the embedding model to correctly interpret the semantic units within the text. Without proper segmentation, the model might treat syllables or parts of compound words as independent entities, leading to inaccurate vector representations that do not capture the true meaning of the text. This requirement directly necessitates the use of a Vietnamese word segmenter, making the underthesea.word_tokenize function a core component of the preprocessing pipeline.
Finally, the user's documents will originate from Google Docs and Google Sheets. This specifies the initial stage of the preprocessing workflow: data extraction. Strategies must be implemented to programmatically access and retrieve the textual content from these platforms in a format that can be readily processed by the subsequent NLP steps. Google Docs, being a word processing application, might contain rich text formatting, while Google Sheets store data in a tabular format. The extraction methods must be able to handle these different structures and retrieve the relevant text content for the RAG application.
Data Extraction and Initial Handling
The first step in the preprocessing pipeline involves extracting the text data from its source, which, in this case, are Google Docs and Google Sheets. Utilizing the respective Google APIs provides the most reliable and efficient way to accomplish this programmatically.
For Google Docs, the Google Docs API (Application Programming Interface) allows developers to interact with documents stored in Google Drive 7. To use this API, it is necessary to set up a project in the Google Cloud Console, enable the Google Docs API for that project, configure the OAuth consent screen to manage user authorization, and obtain the necessary credentials for authentication 8. Once these steps are completed, the API can be used to retrieve the content of a Google Doc given its unique document ID. The API offers options to extract the document content in various formats, including plain text 7. For a RAG application focused on semantic understanding, extracting the plain text content is generally preferred to avoid processing any formatting information that might not contribute to the meaning. The google-api-python-client library provides a convenient way to interact with the Google Docs API using Python 7. A basic workflow involves authenticating the client using the obtained credentials and then making a request to the API to retrieve the document content based on its ID. It is important to implement proper error handling to manage potential issues during API calls and to be mindful of the API's usage quotas to prevent service disruptions.
Similarly, for Google Sheets, the Google Sheets API enables programmatic access to spreadsheet data 13. The setup process in the Google Cloud Console is analogous to that of the Google Docs API: enabling the Google Sheets API and obtaining the necessary authentication credentials, which can include API keys or OAuth 2.0 credentials 13. To extract data, the API requires specifying the ID of the target spreadsheet and the range of cells or sheets to retrieve 13. Google Sheets store data in a structured, tabular format, so the extraction process might yield data in a grid-like structure. Depending on the specific information needed for the RAG application, this tabular data might need to be flattened or processed to extract the relevant textual content. The google-api-python-client library also supports interaction with the Google Sheets API in Python 17. The process involves authenticating and then making requests to the API to read values from the specified spreadsheet and range. Handling different data types within the spreadsheet cells and appropriately structuring the extracted text for further processing will be important considerations.
After successfully extracting the text content from both Google Docs and Google Sheets, an initial data cleaning and preparation phase is recommended. This might involve addressing character encoding issues to ensure consistency across all documents. Removing any irrelevant metadata or boilerplate text that might have been extracted along with the main content, such as headers, footers, or table of contents, can also be beneficial. Furthermore, converting the extracted text to a standard encoding format, such as UTF-8, will help prevent issues in the subsequent NLP processing steps. These initial cleaning tasks ensure that the data is in a consistent and suitable format for the more advanced preprocessing stages.
Vietnamese Text Normalization using Underthesea
Text normalization plays a crucial role in Vietnamese NLP by addressing the inherent variations that can occur in written text 19. These variations can stem from typos, inconsistencies in the use of diacritics (tone marks), different input methods, and the presence of special characters. The goal of normalization is to reduce these inconsistencies, ensuring that semantically equivalent words are represented in a uniform manner. This standardization is vital for improving the accuracy and effectiveness of downstream NLP tasks, including word segmentation and the generation of meaningful embeddings.
Common normalization tasks for Vietnamese text include diacritic standardization, which involves ensuring a consistent representation of the five tones and the absence of tone marks (for the base tone); spelling correction to identify and rectify common misspellings and typographical errors; case handling, although the case sensitivity of the vietnamese-bi-encoder should be investigated, converting text to lowercase can sometimes be beneficial for reducing vocabulary size and improving matching; and punctuation handling, which might involve standardizing or removing punctuation marks based on the requirements of the embedding model and the specific RAG tasks 1.
The underthesea library provides a convenient and effective tool for addressing many of these normalization needs through its text_normalize function 1. This function is specifically designed to handle common Vietnamese text normalization tasks, including correcting diacritic errors and standardizing spelling. For example, as demonstrated in the library's documentation, the input string "Ðảm baỏ chất lựơng phòng thí nghịêm hoá học" is transformed into the normalized form "Đảm bảo chất lượng phòng thí nghiệm hóa học" 1. This function can be readily integrated into the preprocessing pipeline as an initial normalization step.
Beyond text_normalize, the underthesea toolkit might offer other functionalities that can further aid in the normalization process 26. Exploring the library's documentation 25 for modules that handle special characters or case conversion could be valuable. Depending on the specific characteristics of the data extracted from Google Docs and Sheets, there might also be a need for more advanced or custom normalization rules. These could be implemented using regular expressions 21 to address specific patterns of errors or variations that are prevalent in the dataset. For instance, if the data contains inconsistencies in spacing around punctuation, regular expressions can be used to standardize this. The choice of normalization techniques should be guided by the specific types of variations observed in the data and the requirements of the subsequent processing steps.
Vietnamese Word Segmentation using Underthesea
Accurate word segmentation is a fundamental prerequisite for the vietnamese-bi-encoder to effectively process and understand Vietnamese text 4. The quality of the word segmentation directly influences the accuracy of the embeddings generated by the model. If words are not correctly identified and segmented, the embedding model might learn flawed representations, leading to poor semantic matching and retrieval performance in the RAG application.
The underthesea library provides the word_tokenize function, which is specifically designed for segmenting Vietnamese text into individual words 1. This function takes a string of Vietnamese text as input and returns a list of segmented words by default. It also offers a format="text" option that returns a string with the segmented words joined by underscores or spaces 1. For example, the sentence "Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò" is segmented into the list `` 1.
A particularly useful feature of word_tokenize is the fixed_words parameter 1. This allows the user to provide a list of multi-word expressions or named entities that should be treated as single tokens during the segmentation process. This is crucial for preserving the meaning of phrases like "Viện Nghiên Cứu" (Research Institute) or "học máy" (machine learning), which should not be split into their constituent words for accurate semantic representation 1. By identifying and providing such fixed words relevant to the domain of the Google Docs and Sheets data, the accuracy of word segmentation can be significantly improved.
Vietnamese word segmentation can be challenging due to the presence of compound words and multi-word expressions 19. These linguistic units often convey a meaning that is more than the sum of their individual components. While underthesea is trained on Vietnamese corpora and incorporates rules and statistical models to handle many of these cases, achieving perfect segmentation can be difficult. For specialized domains or with specific technical terminology present in the user's documents, it might be necessary to refine the segmentation by providing custom lists of fixed_words or by exploring more advanced techniques if the default performance of underthesea is insufficient.
Research suggests that underthesea employs a hybrid approach to word segmentation, likely combining rule-based methods with statistical models trained on Vietnamese text data 1. These methods often involve identifying potential word boundaries based on linguistic rules and then using statistical language models to disambiguate between different possible segmentations. Techniques like maximal matching, where the longest possible valid words are identified, are also likely employed 28. While a deep dive into the exact algorithms used by underthesea might require examining the library's source code or related research papers, understanding the general principles behind its operation can be helpful for troubleshooting and for deciding if further customization or alternative tools might be necessary.
Strategies for Handling the Maximum Sequence Length Constraint
Given the strict maximum sequence length of 128 tokens for the vietnamese-bi-encoder, effective strategies for handling potentially long input texts are crucial. The first step in this process is typically sentence segmentation.
Underthesea provides the sent_tokenize function for dividing a block of Vietnamese text into individual sentences 1. This function analyzes the text and identifies sentence boundaries based on punctuation marks and other linguistic cues. For example, it can split a text containing multiple sentences into a list where each element is a sentence 1. Sentence segmentation is a logical first step as it breaks down large documents into smaller, semantically coherent units. The subsequent word segmentation and embedding processes can then be applied to these individual sentences. However, it is important to recognize that even after sentence segmentation, some sentences, particularly in technical or legal documents, can still be quite lengthy and might exceed the 128-token limit after being segmented into words 2. Therefore, further chunking might be necessary.
Text chunking involves breaking down longer sequences of text into smaller segments. Several chunking strategies are commonly used in RAG applications 37. One straightforward approach is fixed-size chunking with overlap 37. This method involves dividing the word-segmented text into chunks, each containing a predefined maximum number of tokens (considerably less than 128 to account for variations in sentence length). To maintain context across these chunks, a certain number of tokens from the end of one chunk are repeated at the beginning of the next chunk, creating an overlap. The key parameters for this strategy are the chunk_size (the maximum number of tokens per chunk) and the chunk_overlap (the number of overlapping tokens). These parameters need to be carefully chosen and might require experimentation to find the optimal balance between context retention and redundancy 37.
Another strategy is recursive character text splitting 37. This more adaptive method involves splitting the text using a hierarchy of separators, such as paragraphs, sentences, and words, in a recursive manner until the chunks meet the desired size criteria. This approach attempts to respect the natural semantic boundaries within the text, resulting in more contextually coherent chunks compared to fixed-size splitting. While underthesea might not offer a direct implementation of recursive splitting, the logic can be implemented programmatically, or libraries like LangChain, which are often used in conjunction with tools like underthesea, can be employed for this purpose 40.
Finally, semantic chunking involves splitting text based on the semantic similarity of its content 38. This approach aims to group together text segments that are closely related in meaning. Implementing semantic chunking typically involves using embeddings themselves to identify natural breaks in the semantic flow of the text. While this method has the potential to create the most contextually relevant chunks for retrieval, it is also generally more complex to implement and might require using the vietnamese-bi-encoder or another embedding model to guide the chunking process.
Proposed Preprocessing Pipeline for Your Vietnamese RAG Application
Based on the analysis of the requirements and available tools, a comprehensive preprocessing pipeline for the Vietnamese RAG application can be outlined as follows:
Data Extraction: The process begins with extracting the raw text content from the Google Docs and Google Sheets using their respective APIs. This will involve setting up the necessary Google Cloud project, enabling the APIs, handling authentication, and writing code (potentially in Python using the google-api-python-client library) to retrieve the text content based on the document and spreadsheet IDs.
Initial Cleaning: After extraction, perform basic cleaning operations on the text data. This might include handling character encoding issues, removing irrelevant metadata or boilerplate text, and standardizing the text format (e.g., to UTF-8).
Text Normalization: Apply the underthesea.text_normalize function to the cleaned text to correct diacritic errors and standardize spelling. Depending on the specific characteristics of the data, additional custom normalization steps using regular expressions or other techniques might be necessary.
Sentence Segmentation: Use the underthesea.sent_tokenize function to divide the normalized text into individual sentences. This provides an initial level of granularity for further processing.
Word Segmentation: Employ the underthesea.word_tokenize function to segment each sentence into a sequence of words. Consider creating and using a list of fixed_words for any known multi-word expressions or named entities relevant to the content of the Google Docs and Sheets.
Chunking: Implement a suitable chunking strategy to ensure that each text chunk, after word segmentation, does not exceed the 128-token limit of the vietnamese-bi-encoder. A reasonable starting point could be fixed-size chunking with an overlap. For example, aim for a chunk size of around 100-110 tokens after word segmentation and an overlap of 20-30 tokens. Alternatively, explore implementing recursive character text splitting, using sentence boundaries as a primary separator. The choice of strategy and parameters should be guided by experimentation and evaluation.
The optimal chunking strategy might depend on the specific nature and structure of the content in the Google Docs and Sheets. For highly structured documents, recursive splitting that respects headings or other structural elements could be beneficial. For simpler, more uniform text, fixed-size chunking might be sufficient. It is strongly recommended to experiment with different chunking strategies and parameter settings on a representative sample of the data and to evaluate the performance of the RAG application to determine the most effective approach for the user's specific needs.
Key Considerations and Recommendations
Several key considerations and recommendations should guide the implementation of the proposed preprocessing strategy.
The choices made during each step of the preprocessing pipeline will have a direct impact on the quality of the embeddings generated by the vietnamese-bi-encoder and, ultimately, on the retrieval performance of the RAG application. Therefore, each step should be carefully considered and potentially optimized through experimentation.
When using fixed-size chunking, determining the optimal chunk size and overlap is crucial. Starting with a chunk size that leaves a safety margin below the 128-token limit (e.g., aiming for around 100-110 tokens after word segmentation) and an overlap of 20-30 tokens is a reasonable approach. These values should then be iteratively adjusted based on the evaluation of the RAG application's performance. Too small a chunk size might lead to a loss of necessary context, while too large a size risks exceeding the model's input limit. Similarly, insufficient overlap might break semantic connections between chunks, whereas excessive overlap can introduce unnecessary redundancy and increase processing time.
It is also important to consider the potential for out-of-vocabulary (OOV) words after word segmentation. These are words that are not present in the vocabulary of the vietnamese-bi-encoder. OOV words might not be well-represented by the embedding model, potentially affecting retrieval accuracy if these words are semantically significant. Investigating whether the bi-encoder uses subword tokenization could provide insights into how it handles OOV words. Further normalization or exploring techniques like stemming (with caution, as aggressive stemming might not align with the embedding model's training) could be considered. In cases where OOV words pose a significant problem, fine-tuning the embedding model on the user's specific vocabulary might be an option, although this is a more advanced step.
Finally, it is paramount to emphasize the importance of experimentation and rigorous evaluation. The optimal preprocessing strategy and chunking parameters are likely to be specific to the user's data and the nature of their queries. Therefore, it is recommended to test different approaches on a representative subset of the Google Docs and Sheets data and to evaluate the performance of the RAG application using relevant metrics. This iterative process of experimentation and evaluation will be key to finding the most effective preprocessing configuration for achieving the desired results.
Conclusion: Towards an Effective Vietnamese RAG Application
In conclusion, developing an effective Vietnamese Retrieval-Augmented Generation (RAG) application using underthesea and the bkai-foundation-models/vietnamese-bi-encoder requires a well-designed and carefully implemented text preprocessing strategy. This strategy must encompass thorough data extraction from Google Docs and Sheets, comprehensive text normalization and accurate word segmentation using the underthesea toolkit, and effective chunking techniques to adhere to the embedding model's 128-token limit.
Throughout the preprocessing pipeline, it is crucial to consider the specific characteristics of the user's data and the requirements of the chosen embedding model. Experimentation and evaluation are essential to optimize the various preprocessing steps and to find the most suitable configuration for the RAG application. By following the proposed strategies and continuously refining them based on empirical results, the user can build a robust and high-performing Vietnamese RAG application capable of leveraging external knowledge for enhanced language model capabilities.
Works cited
1. Underthesea - Vietnamese NLP Toolkit - GitHub, accessed March 15, 2025, https://github.com/undertheseanlp/underthesea
2. undertheseanlp - Hugging Face, accessed March 15, 2025, https://huggingface.co/undertheseanlp
3. underthesea - PyPI, accessed March 15, 2025, https://pypi.org/project/underthesea/
4. Bkai-foundation-models - Find Top AI Models on Hugging Face - AIModels.fyi, accessed March 15, 2025, https://www.aimodels.fyi/creators/huggingFace/bkai-foundation-models
5. vietnamese-bi-encoder | AI Model Details - AIModels.fyi, accessed March 15, 2025, https://www.aimodels.fyi/models/huggingFace/vietnamese-bi-encoder-bkai-foundation-models
6. Vietnamese Bi Encoder · Models - Dataloop, accessed March 15, 2025, https://dataloop.ai/library/model/bkai-foundation-models_vietnamese-bi-encoder/
7. Extract the text from a document with Docs API - Google for Developers, accessed March 15, 2025, https://developers.google.com/docs/api/samples/extract-text
8. How to Get Document Texts with the Google Docs API in Python | Endgrate, accessed March 15, 2025, https://endgrate.com/blog/how-to-get-document-texts-with-the-google-docs-api-in-python
9. Using the Google Docs API to Get Document Texts (with Javascript examples) - Endgrate, accessed March 15, 2025, https://endgrate.com/blog/using-the-google-docs-api-to-get-document-texts-(with-javascript-examples)
10. Google Docs API samples, accessed March 15, 2025, https://developers.google.com/docs/api/samples
11. Reading text from Gdoc for feeding to ChatGPT - Help - Pipedream, accessed March 15, 2025, https://pipedream.com/community/t/reading-text-from-gdoc-for-feeding-to-chatgpt/7087
12. Python quickstart | Google Docs, accessed March 15, 2025, https://developers.google.com/docs/api/quickstart/python
13. Extracting data from Google Sheets via API - Sharperlight, accessed March 15, 2025, https://www.sharperlight.com/advanced/2022/04/06/accessing-the-google-sheets-api-via-sharperlight-query-builder/
14. Extract data from smart chips in your Google Sheets - Google Docs Editors Help, accessed March 15, 2025, https://support.google.com/docs/answer/13524011?hl=en
15. Basic reading | Google Sheets, accessed March 15, 2025, https://developers.google.com/sheets/api/samples/reading
16. Read and Write Data in Google Sheets using Python and the Google Sheets API, accessed March 15, 2025, https://aryanirani123.medium.com/read-and-write-data-in-google-sheets-using-python-and-the-google-sheets-api-6e206a242f20
17. Python quickstart | Google Sheets, accessed March 15, 2025, https://developers.google.com/sheets/api/quickstart/python
18. Accessing Google Sheet Data with Python: A Practical Guide using the Google Sheets API, accessed March 15, 2025, https://medium.com/@techworldthink/accessing-google-sheet-data-with-python-a-practical-guide-using-the-google-sheets-api-dc57759d387a
19. Underthesea Vietnamese NLP Toolkit | Restackio, accessed March 15, 2025, https://www.restack.io/p/vietnamese-nlp-tools-answer-underthesea-cat-ai
20. Vietnamese Text Recognition Dataset | Restackio, accessed March 15, 2025, https://www.restack.io/p/vietnamese-nlp-tools-answer-text-recognition-dataset-cat-ai
21. Vietnamese Sentiment Analysis - Kaggle, accessed March 15, 2025, https://www.kaggle.com/code/tonibui3107/vietnamese-sentiment-analysis
22. Nlp Cho Tiếng Việt - Vietnamese Nlp Tools | Restackio, accessed March 15, 2025, https://www.restack.io/p/vietnamese-nlp-tools-answer-nlp-cho-tieng-viet-cat-ai
23. NeMo-text-processing/nemo_text_processing/text_normalization/normalize.py at main - GitHub, accessed March 15, 2025, https://github.com/NVIDIA/NeMo-text-processing/blob/main/nemo_text_processing/text_normalization/normalize.py
24. Chuẩn hóa văn bản - ProtonX, accessed March 15, 2025, https://protonx.io/courses/66487737f91fdc001a81ce3a/topics/665057c88c287e0019bb800b
25. Underthesea documentation — Under The Sea 1.1.9 documentation, accessed March 15, 2025, https://underthesea.readthedocs.io/
26. Underthesea v6.6.0 [Latest Version] - Colab, accessed March 15, 2025, https://colab.research.google.com/drive/1gD8dSMSE_uNacW4qJ-NSnvRT85xo9ZY2
27. Vietnamese NLP Toolkit — Under The Sea 1.1.9 ... - Underthesea, accessed March 15, 2025, https://underthesea.readthedocs.io/en/latest/readme.html
28. A Hybrid Approach to Word Segmentation of Vietnamese Texts - ResearchGate, accessed March 15, 2025, https://www.researchgate.net/publication/29616221_A_Hybrid_Approach_to_Word_Segmentation_of_Vietnamese_Texts
29. Is word segmentation necessary for Vietnamese sentiment classification? - arXiv, accessed March 15, 2025, https://arxiv.org/pdf/2301.00418
30. NLP-Vietnamese-progress/tasks/word_segmentation.md at master - GitHub, accessed March 15, 2025, https://github.com/undertheseanlp/NLP-Vietnamese-progress/blob/master/tasks/word_segmentation.md
31. A Large-Scale Benchmark for Vietnamese Sentence Paraphrases - arXiv, accessed March 15, 2025, https://arxiv.org/html/2502.07188v1
32. NLP Benchmarking popular Vietnamese tokenizer - Huy Bik's Blog, accessed March 15, 2025, https://huybik.github.io/Word-Tokenizer-Benchmark/
33. undertheseanlp/sent_tokenize: Vietnamese Sentence ... - GitHub, accessed March 15, 2025, https://github.com/undertheseanlp/sent_tokenize
34. sentence-segmentation · GitHub Topics, accessed March 15, 2025, https://github.com/topics/sentence-segmentation?o=desc&s=forks
35. underthesea/tox.ini at main - GitHub, accessed March 15, 2025, https://github.com/undertheseanlp/underthesea/blob/main/tox.ini
36. AttributeError: module 'pytest' has no attribute 'mark' · Issue #303 · undertheseanlp/underthesea - GitHub, accessed March 15, 2025, https://github.com/undertheseanlp/underthesea/issues/303
37. 7 Chunking Strategies in RAG You Need To Know - F22 Labs, accessed March 15, 2025, https://www.f22labs.com/blogs/7-chunking-strategies-in-rag-you-need-to-know/
38. 11 Chunking Strategies for RAG — Simplified & Visualized | by Mastering LLM (Large Language Model), accessed March 15, 2025, https://masteringllm.medium.com/11-chunking-strategies-for-rag-simplified-visualized-df0dbec8e373
39. A Guide to Chunking Strategies for Retrieval Augmented Generation (RAG) - Sagacify, accessed March 15, 2025, https://www.sagacify.com/news/a-guide-to-chunking-strategies-for-retrieval-augmented-generation-rag
40. Five Levels of Chunking Strategies in RAG| Notes from Greg's Video | by Anurag Mishra, accessed March 15, 2025, https://medium.com/@anuragmishra_27746/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d
41. Effective Chunking Strategies for RAG - Cohere Documentation, accessed March 15, 2025, https://docs.cohere.com/v2/page/chunking-strategies
42. Simple Chunking Strategies for RAG Applications (Part 1) | by kirouane Ayoub | Medium, accessed March 15, 2025, https://medium.com/@ayoubkirouane3/simple-chunking-strategies-for-rag-applications-part-1-d56903b167c5
43. Chunking strategies for RAG tutorial using Granite - IBM, accessed March 15, 2025, https://www.ibm.com/think/tutorials/chunking-strategies-for-rag-with-langchain-watsonx-ai
